{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0506099fd1553aa4da42d83c60d28745b1ff66d9527e63b0b6cab9a89f7c2a048",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "np.random.seed(1010)\n",
    "\n",
    "RND_MEAN = 0\n",
    "RND_STD = 0.003\n",
    "\n",
    "LEARNING_RATE = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 : Train - loss = 33.823, accuracy = 0.560 / Test = 0.811\n",
      "Epoch 2 : Train - loss = 8.076, accuracy = 0.820 / Test = 0.808\n",
      "Epoch 3 : Train - loss = 7.444, accuracy = 0.812 / Test = 0.804\n",
      "Epoch 4 : Train - loss = 7.343, accuracy = 0.811 / Test = 0.801\n",
      "Epoch 5 : Train - loss = 7.267, accuracy = 0.810 / Test = 0.802\n",
      "Epoch 6 : Train - loss = 7.202, accuracy = 0.811 / Test = 0.801\n",
      "Epoch 7 : Train - loss = 7.147, accuracy = 0.810 / Test = 0.801\n",
      "Epoch 8 : Train - loss = 7.101, accuracy = 0.810 / Test = 0.802\n",
      "Epoch 9 : Train - loss = 7.056, accuracy = 0.812 / Test = 0.799\n",
      "Epoch 10 : Train - loss = 7.022, accuracy = 0.810 / Test = 0.800\n",
      "\n",
      " Result : final accuracy = 0.800\n"
     ]
    }
   ],
   "source": [
    "# 앞서 작성한 회귀 모델을 실행\n",
    "%run ./Neural_Network_Regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification_exec(epoch_count = 10, mb_size = 10, report = 1, train_rate = 0.75):\n",
    "    binary_load_dataset()\n",
    "    init_model()\n",
    "    train_and_test(epoch_count, mb_size, report, train_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 과정\n",
    "def binary_load_dataset():\n",
    "    with open('./data/pulsar_stars.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader, None)\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "\n",
    "    global data,input_cnt,output_cnt\n",
    "\n",
    "    input_cnt, output_cnt = 8,1 # 입출력 벡터 수\n",
    "    data = np.asarray(rows, dtype='float32') #현재 리스트 구조의 변수를 배열구조로 변환 -> numpy 연산을 효율적으로 사용하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파 과정\n",
    "def forward_postproc(output, y):\n",
    "    CEE = sigmoid_cross_entropy_with_logits(y,output) # 시그모이드 교차 엔트로피 함수\n",
    "    loss = np.mean(CEE)\n",
    "\n",
    "    return loss, [y, output, CEE] # 리스트 : 역전파 과정을 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수에 대한 역전파\n",
    "def backprop_postproc(G_loss, aux):\n",
    "    y, output, CEE = aux\n",
    "    G_loss = 1.0\n",
    "\n",
    "    g_loss_entropy = 1.0 / np.prod(CEE.shape)\n",
    "    g_entropy_output = sigmoid_cross_entropy_with_logits_derv(y,output)\n",
    "\n",
    "    G_entropy = g_loss_entropy * G_loss\n",
    "    G_output = g_entropy_output * G_entropy\n",
    "\n",
    "    return G_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 관련 함수 정의\n",
    "\n",
    "# 0 과 x 중 큰 값을 출력\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "# 시그모이드 함수\n",
    "def sigmoid(x):\n",
    "    return np.exp(-relu(-x)) / (1 + np.exp(-np.abs(x)))\n",
    "\n",
    "# 시그모이드 함수의 편미분\n",
    "def sigmoid_derv(x,y):\n",
    "    return y *(1-y)\n",
    "\n",
    "# 시그모이드 교차 엔트로피 \n",
    "def sigmoid_cross_entropy_with_logits(z,x):\n",
    "    return relu(x) - x * z + np.log(1+np.exp(-np.abs(x)))\n",
    "\n",
    "# 시그모이드 교차 엔트로피의 편미분( x에 대한 편미분으로 진행))\n",
    "def sigmoid_cross_entropy_with_logits_derv(z,x):\n",
    "    return -z + sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 문제에 대한 정확도 판단\n",
    "def eval_accuracy(output,y):\n",
    "    # np.greater(a,b) : a 가 b보다 크면 1(True), 작다면 0(False)\n",
    "    estimate = np.greater(output,0)\n",
    "    answer = np.greater(y,0.5)\n",
    "    correct = np.equal(estimate, answer)\n",
    "\n",
    "    return np.mean(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 : Train - loss = 0.140, accuracy = 0.961 / Test = 0.969\n",
      "Epoch 2 : Train - loss = 0.127, accuracy = 0.967 / Test = 0.965\n",
      "Epoch 3 : Train - loss = 0.125, accuracy = 0.967 / Test = 0.972\n",
      "Epoch 4 : Train - loss = 0.134, accuracy = 0.968 / Test = 0.970\n",
      "Epoch 5 : Train - loss = 0.128, accuracy = 0.970 / Test = 0.972\n",
      "Epoch 6 : Train - loss = 0.124, accuracy = 0.970 / Test = 0.970\n",
      "Epoch 7 : Train - loss = 0.107, accuracy = 0.971 / Test = 0.971\n",
      "Epoch 8 : Train - loss = 0.118, accuracy = 0.971 / Test = 0.969\n",
      "Epoch 9 : Train - loss = 0.121, accuracy = 0.970 / Test = 0.973\n",
      "Epoch 10 : Train - loss = 0.129, accuracy = 0.970 / Test = 0.971\n",
      "\n",
      " Result : final accuracy = 0.971\n"
     ]
    }
   ],
   "source": [
    "binary_classification_exec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "위의 결과는 상당한 결과지만, 사실 착시현상(?)이 존재한다\n",
    "알고자 하는 Target_class의 데이터 불균형으로 인한 결과..( 0 : 16000개, 1 : 1600개 )\n",
    "따라서 균형을 위해 데이터의 증폭이 필요 -> 일부 함수의 수정한다..!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 함수를 일부 수정한다 / adjust_ratio : 데이터 증폭 여부에 대한 인자\n",
    "def binary_classification_exec(epoch_count = 10, mb_size = 10, report = 1, train_rate = 0.75,adjust_ratio = False):\n",
    "    binary_load_dataset(adjust_ratio)\n",
    "    init_model()\n",
    "    train_and_test(epoch_count, mb_size, report, train_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 과정 + 증촉 여부에 따른 추가 전처리 과정\n",
    "def binary_load_dataset(adjust_ratio):\n",
    "    pulsars, stars = [], []         # 데이터를 따로 저장하기 위한 빈 객체 생성\n",
    "    with open('./data/pulsar_stars.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader, None)\n",
    "\n",
    "        for row in csvreader:       # 이전에는 한 번에 저장했다면, 이번엔 불균형의 열을 따로 저장\n",
    "            if row[8] == '1' :      # target_class 부분을 따로 데이터 저장\n",
    "                pulsars.append(row)\n",
    "            else:\n",
    "                stars.append(row)   # 그 외 데이터 저장\n",
    "\n",
    "    global data,input_cnt,output_cnt\n",
    "\n",
    "    input_cnt, output_cnt = 8,1 # 입출력 벡터 수\n",
    "    star_cnt, pulsar_cnt = len(stars), len(pulsars)\n",
    "    if adjust_ratio:    # adjust_ratio 가 참일 경우(데이터 증폭을 진행할 경우)\n",
    "        data = np.zeros([2*star_cnt,9]) # 기존의 data 버퍼 크기보다 크게 설정(행 길이 2배, 열은 9개를 의미)\n",
    "        data[0:star_cnt,:] = np.asarray(stars,dtype='float32')  # 위 크기의 절반은 stars 데이터\n",
    "\n",
    "        for n in range(star_cnt):   # 나머지 절반은 pulsars 데이터 저장\n",
    "            data[star_cnt + n] = np.asarray(pulsars[n % pulsar_cnt], dtype='float32')\n",
    "\n",
    "\n",
    "    else:   # adjust_ratio 가 거짓이라면, 이전과 동일하게 저장\n",
    "        data = np.zeros([star_cnt + pulsar_cnt, 9])\n",
    "        data[0:star_cnt,:] = np.asarray(stars,dtype='float32')\n",
    "        data[star_cnt:,:] = np.asarray(pulsar_cnt,dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 문제에 대한 정확도 판단\n",
    "def eval_accuracy(output,y):\n",
    "    # np.greater(a,b) : a 가 b보다 크면 1(True), 작다면 0(False)\n",
    "    est_yes = np.greater(output,0)\n",
    "    ans_yes = np.greater(y,0.5)\n",
    "\n",
    "    est_no = np.logical_not(est_yes)\n",
    "    ans_no = np.logical_not(ans_yes)\n",
    "\n",
    "    TP = np.sum(np.logical_and(est_yes,ans_yes))\n",
    "    FP = np.sum(np.logical_and(est_yes,ans_no))\n",
    "    FN = np.sum(np.logical_and(est_no,ans_yes))\n",
    "    TN = np.sum(np.logical_and(est_no,ans_no))\n",
    "\n",
    "    accuracy = safe_div(TP+TN, TP+FP+FN+TN)\n",
    "    precision = safe_div(TP,TP+FP)\n",
    "    recall = safe_div(TP,TP+FN)\n",
    "\n",
    "    f1 = 2 * safe_div(recall * precision, recall + precision)\n",
    "    \n",
    "\n",
    "    return [accuracy, precision, recall, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분모가 0일 경우, 분자의 부호에 맞는 부호로 변환\n",
    "def safe_div(p,q):\n",
    "    # 형 변환\n",
    "    p, q = float(p), float(q)\n",
    "    if np.abs(q) < 1.0e-20:\n",
    "        return np.sign(p)\n",
    "    \n",
    "    return p / q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가\n",
    "def train_and_test(epoch_count, mb_size, report, train_rate):\n",
    "    step_count = arrange_data(mb_size, train_rate)\n",
    "    test_x, test_y = get_test_data()\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        losses = []\n",
    "\n",
    "        for n in range(step_count):\n",
    "            train_x, train_y= get_train_data(mb_size, n)\n",
    "            loss, _ = run_train(train_x,train_y)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        if report > 0 and (epoch + 1) % report == 0:\n",
    "            acc = run_test(test_x, test_y)\n",
    "            print(\"Epoch {} : Train - loss = {:5.3f}, accuracy = {:5.3f}, precision = {:5.3f}, recall = {:5.3f}, f1 = {:5.3f}\".\\\n",
    "                format(epoch+1, np.mean(losses), acc[0], acc[1], acc[2], acc[3]))\n",
    "    \n",
    "    final_acc = run_test(test_x,test_y)\n",
    "    print('\\n Result : final accuracy = {:5.3f}, precision = {:5.3f}, recall = {:5.3f}, f1 = {:5.3f}'.format(final_acc[0],final_acc[1],final_acc[2],final_acc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 : Train - loss = -63298719093271.656, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 200 : Train - loss = -126916015462562.141, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 300 : Train - loss = -190533289488288.000, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 400 : Train - loss = -254150581130592.062, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 500 : Train - loss = -317767880507534.875, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 600 : Train - loss = -381385182032679.500, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 700 : Train - loss = -445002453480455.812, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 800 : Train - loss = -508619739107502.375, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 900 : Train - loss = -572237021726713.500, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "Epoch 1000 : Train - loss = -635854305635235.250, accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n",
      "\n",
      " Result : final accuracy = 0.094, precision = 0.094, recall = 1.000, f1 = 0.172\n"
     ]
    }
   ],
   "source": [
    "binary_classification_exec(epoch_count=1000, report=100, mb_size=10, adjust_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 : Train - loss = 0.342, accuracy = 0.914, precision = 0.984, recall = 0.839, f1 = 0.906\n",
      "Epoch 200 : Train - loss = 0.351, accuracy = 0.930, precision = 0.977, recall = 0.879, f1 = 0.926\n",
      "Epoch 300 : Train - loss = 0.323, accuracy = 0.930, precision = 0.945, recall = 0.912, f1 = 0.928\n",
      "Epoch 400 : Train - loss = 0.318, accuracy = 0.919, precision = 0.988, recall = 0.847, f1 = 0.912\n",
      "Epoch 500 : Train - loss = 0.310, accuracy = 0.931, precision = 0.980, recall = 0.880, f1 = 0.927\n",
      "Epoch 600 : Train - loss = 0.309, accuracy = 0.939, precision = 0.964, recall = 0.911, f1 = 0.937\n",
      "Epoch 700 : Train - loss = 0.329, accuracy = 0.936, precision = 0.972, recall = 0.897, f1 = 0.933\n",
      "Epoch 800 : Train - loss = 0.314, accuracy = 0.932, precision = 0.942, recall = 0.919, f1 = 0.930\n",
      "Epoch 900 : Train - loss = 0.310, accuracy = 0.928, precision = 0.928, recall = 0.926, f1 = 0.927\n",
      "Epoch 1000 : Train - loss = 0.297, accuracy = 0.935, precision = 0.968, recall = 0.899, f1 = 0.932\n",
      "\n",
      " Result : final accuracy = 0.935, precision = 0.968, recall = 0.899, f1 = 0.932\n"
     ]
    }
   ],
   "source": [
    "binary_classification_exec(epoch_count=1000, report=100, mb_size=10, adjust_ratio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}